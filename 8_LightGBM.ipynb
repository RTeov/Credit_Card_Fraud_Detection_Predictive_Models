{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a6094",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = os.getcwd()\n",
    "print(working_directory)\n",
    "data = pd.read_csv(f\"{working_directory}/Input_Data/creditcard_post_correlation.csv\") #Change the path to your dataset, if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438a62f",
   "metadata": {},
   "source": [
    "## Define Predictors and Target Variables\n",
    "##### We will specify the predictor features and the target variable. Additionally, categorical features can be identified if present. In this case, there are no categorical features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b45ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "target = 'Fraud_Flag'\n",
    "\n",
    "# Define the features to be used in the model\n",
    "predictors = [\n",
    "    'Transaction_Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "    'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\n",
    "    'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\n",
    "    'Transaction_Amount'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009ba76",
   "metadata": {},
   "source": [
    "## Define the TRAIN/VALIDATION/TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c20dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN/VALIDATION/TEST SPLIT\n",
    "#VALIDATION\n",
    "VALID_SIZE = 0.20 # simple validation using train_test_split\n",
    "TEST_SIZE = 0.20 # test size using_train_test_split\n",
    "\n",
    "#CROSS-VALIDATION\n",
    "NUMBER_KFOLDS = 5 #number of KFolds for cross-validation\n",
    "\n",
    "RANDOM_STATE = 2018\n",
    "\n",
    "MAX_ROUNDS = 1000 #lgb iterations\n",
    "EARLY_STOP = 50 #lgb early stop \n",
    "OPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\n",
    "VERBOSE_EVAL = 50 #Print out metric result\n",
    "\n",
    "# Set the path to the input data\n",
    "IS_LOCAL = True  # Set to True since you we running locally\n",
    "\n",
    "if IS_LOCAL:\n",
    "    PATH = \"C:/Users/teovr/Desktop/Credit_Card_Fraud_Detection_Predictive_Model/Input_Data/\"\n",
    "else:\n",
    "    PATH = \"../input\"\n",
    "\n",
    "print(os.listdir(PATH))  # List the files in the specified directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e13e840",
   "metadata": {},
   "source": [
    "## Split data in train, test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(\n",
    "    data, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Further split the training set into training and validation sets\n",
    "train_df, valid_df = train_test_split(\n",
    "    train_df, \n",
    "    test_size=VALID_SIZE, \n",
    "    random_state=RANDOM_STATE, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c4bb7",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bf19c",
   "metadata": {},
   "source": [
    "#### Continue testing with another gradient boosting algorithm, LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81fbe7",
   "metadata": {},
   "source": [
    "### LightGBM definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\n",
    "NUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\n",
    "NUMBER_OF_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99149659",
   "metadata": {},
   "source": [
    "### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the LightGBM parameters\n",
    "lgbm_parameters = {\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'binary',\n",
    "          'metric':'auc',\n",
    "          'learning_rate': 0.05,\n",
    "          'num_leaves': 7,  # we should let it be smaller than 2^(max_depth)\n",
    "          'max_depth': 4,  # -1 means no limit\n",
    "          'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "          'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "          'subsample': 0.9,  # Subsample ratio of the training instance.\n",
    "          'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "          'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "          'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "          'nthread': 8,\n",
    "          'verbose': 0,\n",
    "          'scale_pos_weight':150, # because training data is extremely unbalanced \n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60b265",
   "metadata": {},
   "source": [
    "### Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e200d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training dataset\n",
    "dtrain = lgb.Dataset(train_df[predictors].values, \n",
    "                     label=train_df[target].values,\n",
    "                     feature_name=predictors)\n",
    "\n",
    "# Create the validation dataset\n",
    "dvalid = lgb.Dataset(valid_df[predictors].values,\n",
    "                     label=valid_df[target].values,\n",
    "                     feature_name=predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c1af4",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bac852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track evaluation results for both training and validation sets\n",
    "eval_result = {}\n",
    "\n",
    "# Create a callback to record evaluation metrics\n",
    "record_cb = lgb.record_evaluation(eval_result)\n",
    "\n",
    "# Train the LightGBM model with verbose evaluation every 50 rounds\n",
    "model = lgb.train(\n",
    "    lgbm_parameters,\n",
    "    dtrain,\n",
    "    valid_sets=[dtrain, dvalid],\n",
    "    valid_names=['train', 'eval'],\n",
    "    callbacks=[record_cb, lgb.early_stopping(stopping_rounds=EARLY_STOP), lgb.log_evaluation(period=VERBOSE_EVAL)]\n",
    ")\n",
    "\n",
    "# Print a summary of the best validation AUC\n",
    "best_iter = model.best_iteration\n",
    "best_auc = eval_result['eval']['auc'][best_iter - 1]\n",
    "print(f\"Best iteration: {best_iter}\")\n",
    "print(f\"Best validation AUC: {best_auc:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead228f",
   "metadata": {},
   "source": [
    "##### Best validation score was obtained for round 29, for which AUC ~= 0.95786."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528bd7f0",
   "metadata": {},
   "source": [
    "### Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LightGBM feature importance with improved style and readability\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "lgb.plot_importance(\n",
    "    model,\n",
    "    ax=ax,\n",
    "    height=0.8,\n",
    "    title=\"Feature Importance (LightGBM)\",\n",
    "    color=\"forestgreen\",\n",
    "    importance_type='gain',  # 'gain' is often more informative than 'split'\n",
    "    max_num_features=15,     # Show top 15 features for clarity\n",
    "    xlabel=\"Importance Score\"\n",
    ")\n",
    "\n",
    "# Add value labels to bars\n",
    "ax.set_xlabel(\"Importance Score\", fontsize=14)\n",
    "ax.set_ylabel(\"Feature\", fontsize=14)\n",
    "ax.set_title(\"Top Feature Importances (LightGBM)\", fontsize=18, fontweight='bold', pad=15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad690e",
   "metadata": {},
   "source": [
    "### Predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions5 = model.predict(test_df[predictors])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b958e",
   "metadata": {},
   "source": [
    "### ROC-AUC score (Area under curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c5860",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(test_df[target].values, predictions5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ad69c",
   "metadata": {},
   "source": [
    "##### The ROC-AUC score obtained for the test set is 0.947."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
